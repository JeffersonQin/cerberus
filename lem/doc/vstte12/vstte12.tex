\documentclass[a4paper]{llncs}


\usepackage[T1]{fontenc}
\usepackage{ae}
\usepackage{hyperref}
\usepackage{lem}
\usepackage{url}

\newcommand{\myparagraph}[1]{\vspace{0.5\baselineskip}\par\noindent{\normalsize\bfseries{#1}}\quad}

%\makeatletter
%\renewcommand{\section}{\vspace*{-2mm}\@startsection{section}{1}{\z@}%
%                       {-18\p@ \@plus -4\p@ \@minus -4\p@}%
%                       {4\p@ \@plus 2\p@ \@minus 2\p@}%
%                       {\normalfont\large\bfseries\boldmath
%                        \rightskip=\z@ \@plus 8em\pretolerance=10000 }}
%\makeatothervtt

\pagestyle{plain}

\newcommand{\Sevcik}{\v{S}ev\v{c}\'{i}k}

\newcommand{\Lem}{\textsc{Lem}}
\newcommand{\Lemsect}{Lem}

\usepackage{todo}

\author{Scott Owens \and Peter B\"ohm \and Mark Batty \and Peter Sewell}
\title{Expressing Large-scale Semantics in Lem}
%Formulating Extensive Semantics in Lem \\
%Expressing Complex Semantics in Lem \\
%Formalising Extensive Semantics in Lem}
\institute{University of Cambridge\\ \url{http://www.cl.cam.ac.uk/~so294/lem/}}


\begin{document}
\sloppy
\maketitle

\begin{abstract} 
%
Trustworthy software verification relies on accurate semantic definitions of
the programming languages and systems that the software runs on; engineering
these definitions for realistic systems is a major activity in its own right.
Thus, in a large-scale verification effort, proof is only one task among many:
testing, simulation, communication, community review, etc., all of which
benefit from mechanization.  Moreover, the effort invested in establishing such
definitions should be re-usable and, where possible, not tied to a single tool.

This paper details \Lem{}, a system specifically designed to support pragmatic
engineering of such definitions.  \Lem's specification language is based on familiar
concepts from functional programming languages and higher-order logics, and
\Lem{} translates specifications to a variety of programming languages and proof
assistants, preserving the original source structure (layout, comments, etc.)
to ensure the result is readable and usable.  We have used \Lem{} to support
our recent work on the formalisation of C++11 concurrency.  Here, we detail how
\Lem{} solved a series of common tool chain issues to give us the flexibility
to use a variety of existing tools (HOL4, Isabelle, OCaml, \LaTeX{}) in our
work.

\end{abstract}

\section{Motivation} \label{sec:motivation} 
%
The ever growing complexity of today's computer systems has established a
demand for machine-checked formalization of verification efforts.  Mechanised
proof assistants, such as ACL2~\cite{ACL2}, Coq~\cite{Coq}, HOL4~\cite{HOL},
HOL Light~\cite{HOLlight}, Isabelle/HOL~\cite{Isabelle}, PVS~\cite{PVS}, and
Twelf~\cite{Twelf} provide extensive support for machine-checked, interactive
and/or automated reasoning, but their main focus is on proof support.  Each has
its own logic, various mechanisms for stating mathematical definitions, and
different methods to formulate semantics in support of the proof.  

Traditionally, the overall verification effort is dominated by proof; however,
when mechanizing large-scale developments, especially those including
real-world programming languages or micro-processors, good definitions are
equally important.  Their engineering is a major activity in its own
right:
%
\begin{itemize}
  \item proof is then only one task among many---testing, simulation,
    communication, community review, etc---which rely on the
    definitions; and
  \item concise, well-engineered definitions can support and reduce the
    verification effort significantly.
\end{itemize}
 
Additionally, in some applications the definitions themselves, of types,
functions, and relations, are a major focus of the work.  This is often the case
when modelling key computational infrastructure: network protocols, programming
languages, multiprocessors, and so on.  For example, we have worked on
TCP~\cite{TCP:paper,RNS08}, Optical Networking~\cite{BDJRS06}, Java Module
Systems~\cite{ljam}, the semantics of an OCaml fragment~\cite{ocamllightesop},
concurrency for C and C++~\cite{C++,ppdp11,Ctso}, and the semantics of x86,
POWER and ARM multiprocessors~\cite{pldi11,x86popl}.  The L4-verified project
defines a micro-kernel, at varying levels of abstraction, in C, Haskell, and
Isabelle/HOL~\cite{l4-verif}.  The Verisoft project defines a micro-kernel in a
subset of C (C0) with inline assembler; the formal semantics of the
kernel~\cite{Alkassar:VSTTE2010} and of a C0 comiler~\cite{Leinenbach:SSV08} are
defined in Isabelle/HOL, exclusively.
 
In each of these cases, considerable effort was required to establish the
definitions of syntax and semantics, including analysis of informal
specifications, empirical testing, and proof of metatheory. These definitions
can be large: for example, our TCP specification is around 10\,000 non-comment
lines of HOL4.  At this scale, the activity of working with the definitions
becomes more like developing software than defining small calculi: one has to
refactor, test, coordinate between multiple people, and so on, and all of this
should, as far as possible, be complete before one embarks on any proof.

Moreover, in such work a proof assistant is just one piece of a complex project,
involving production typesetting, testing infrastructure, code generation, and
tools for embedding source-language terms into the prover.  Sometimes there is
no proof activity, but great benefits arise simply from working in typechecked
and typeset mathematics; sometimes there is mechanised symbolic evaluation or
code generation for testing and prototyping; sometimes there is hand proof or a
mixture of hand and mechanised proof; and sometimes there is the classic full
mechanised proof supported by provers.

Ideally, the results of such work should be made widely available in a
\emph{re-usable} form, so that other groups can build on them and 
the field can eventually converge on standard models for the
relatively stable aspects of the computational environment in which we
work.  Unfortunately, at present such re-use is highly restricted for two
reasons.
%
Firstly, the field is partitioned into schools around each prover: the
difficulty in becoming fluent in their use means that very few people
can use more than one tool effectively.  Indeed, even within some of our
own projects we have had to use several provers due to differing local
expertise.  This variation makes it hard to compare the results of even
carefully specified benchmarks, such as the POPLmark
challenge~\cite{poplmark}.

Secondly, the differences between the provers mean that it is a major and
error-prone task to port a development---or even just its definitions---from one
system to another.  In some cases this is for fundamental reasons:  definitions
which make essential use of the dependent types of Coq may be hard or impossible
to practically port to HOL4.  However, many of the examples cited above are
logically undemanding: they have no need for dependent types, the differences
between classical and constructive reasoning are not particularly relevant, and
there is often little or no object-language variable binding (of course this
does not apply for formalisation of rich type theories).   They do make heavy
use of basic discrete mathematics and ``programming language'' features: sets
and set comprehensions; first-order logic; and inductive types and records with
functions and relations over them.  Thus, the challenge is one of robustly
translating between the concrete syntax and definition styles of the different
proof assistants.


% C++ Motivation
\subsection{Formalising C++ Concurrency}\label{sub:cppmotivation}

In this paper, we detail our use of \Lem{} in formalising the concurrency model of C++~\cite{C++}.
Modern multiprocessors provide shared memory as a communication channel between
their individual cores, but each core implements optimisations which can
visibly affect the shared memory communication, such as buffering, caching, and
speculation.  Compiler optimisations can further complicate the observable
behaviour. To specify the observable effects of the possible reorderings,
processors and progamming languages provide a relaxed interface to memory:
there is no longer a notion of a global ordering of memory reads and writes. A
set of rules, the \emph{memory model}, defines the valid executions of a
program by a processor.  Upcoming versions of the C++ and C languages (C++11
and C1x respectively) implement a relaxed memory model that allows efficient
implementation of the concurrency features running on modern relaxed memory
multiprocessors.

In practice, the rules that make up the C++11 memory model---which C1x
shares---are written in 'Standardese': English prose, with a subset of words
like 'should' and 'shall' taking precise new meanings. The rules are numerous,
intricate, and therefore prone to be incorrect or inconsistent. This combination
of an informal language and an intrinsically complex specification problem makes
such a document an exemplary application area for formal sematics: by
understanding and specifying the document in machine-processed, formal
mathematics, (unintentional) ambiguities are clarified and often first flaws are
already detected before even embarking on any verification effort. Then, the
semantic model can be the basis for further verification challenges, such as
software or compiler verification.


\section{Portable Definitions with \Lemsect}

We have designed \emph{\Lem{}} as a stand-alone, lightweight tool for writing,
managing, and publishing large-scale semantic definitions, for use as a
intermediate language when generating definitions for domain-specific tools, and
for use as a specification language for porting definitions between existing
provers. In recent work~\cite{Lem-ITP11}, we outlined a prototype version of the
tool.

The tool typechecks \Lem{} sources and generates prover code for HOL4~\cite{HOL}
and Isabelle/HOL~\cite{Isabelle}, executable definitions in OCaml~\cite{ocaml},
and \LaTeX{} sources; the latter drawing on Wansbrough's HOLDoc tool design.
Additionally, Coq code generation is currently in development and the tool
design supports the addition of new code targets in a structured, modular way. 

Semantically, \Lem's source language is roughly the intersection of common
functional programming languages and higher-order logics, which we regard as a
sweet spot: expressive enough for the applications we mention above, yet
familiar and relatively easy to translate into the various provers; there is
intentionally no logical novelty here. The language has a simple type
theory~\cite{church40} with primitive support for recursive and higher-order
functions, inductive relations, n-ary tuples, algebraic datatypes, record types,
type inference, and top-level polymorphism.  It also includes a type class
mechanism broadly similar to Isabelle's and Haskell's~\cite{haskell} (without
constructor classes).  It differs from the internal logics of HOL4 or
Isabelle/HOL principally in having type, function, and relation definitions as
part of the language rather than encoded into it: the \Lem{} type system is
formally defined using Ott~\cite{ott} in terms of the user-level syntax.

The novelty is rather in the detailed design and implementation, which ensure
the following four important pragmatic properties. By building a lightweight
stand-alone tool, these goals can be achieved more easily than in the context of
a prover implementation because we are neither constrained by a representation
suitable for a proof kernel implementation (e.g., explicitly typed lambda
terms), nor by a large legacy code base.


\myparagraph{1. Readability of source files}
\Lem{} syntactically resembles OCaml and F\texttt{\#}~\cite{fsharp}, giving us a
popular and readable syntax.  It includes nested modules (but not functors),
recursive type and function definitions, record types, type abbreviations, and
pattern matching.  It has additional syntax for quantifiers, including
restricted quantifiers ($\forall x \in S.\ P x$), set comprehension, and
inductive relations. 

For example, here is an extract from our C++11 concurrency model:
\par\noindent{\footnotesize
\begin{verbatim}
let consistent_modification_order actions lk sb sc mo hb =
  (forall (a IN actions) (b IN actions). (a,b) IN mo 
    --> (same_location a b && is_write a && is_write b)) &&
  (forall (l IN locations_of actions).
    match lk l with 
      Atomic -> ( 
        let actions_at_l = 
          {a | forall (a IN actions) | location_of a = Some l} in
        let writes_at_l = 
          {a | forall (a IN actions_at_l) | is_write a} in
        strict_total_order_over writes_at_l 
          (restrict_relation_set mo actions_at_l) &&
        (* hb is a subset of mo at l *)
        restrict_relation_set hb writes_at_l subset mo &&
        (* SC fences impose mo *)
        (restrict_relation_set
          (compose (compose sb (restrict_relation_set sc 
            {a | forall (a IN actions) | is_fence a})) sb )
          writes_at_l) subset mo)
    | _   -> ( 
        let actions_at_l = 
          {a | forall (a IN actions) | location_of a = Some l} in
        Set.is_empty (restrict_relation_set mo actions_at_l) ) 
    end )
\end{verbatim} }
\noindent We do not always follow OCaml: for example, \Lem{} uses curried data
constructors instead of tupled ones, and it uses \verb+<|+ and \verb+|>+ for
records, saving \verb|{| and \verb|}| for set comprehensions.  Type classes
provide principled support for overloading.

\Lem{} does not at present include support for arbitrary
user-defined syntax, as provided by Ott and (to a greater or lesser
extent) by several proof assistants.
%
\Lem{} and Ott have complementary strengths:  Ott is particularly useful for
defining semantics as inductively defined relations over a rich user syntax, but
has limited support for logic, sets, and function definitions, whereas \Lem{} is
the converse.  We envisage refactoring the Ott implementation, which currently
generates Coq, HOL4, and Isabelle/HOL code separately, to instead generate
\Lem{} code and leave the prover-specific output to the \Lem{} tool.  In the
longer term, a metalanguage that combines both is highly desirable.


\myparagraph{2. Taking the source text seriously}
\emph{Explaining} the definitions is a key aspect of the kind of work we mention
above.  We need to produce production-quality typesetting, of the complete
definitions in logical order and of various excerpts, in papers, longer
documents, and presentations.  As all these have to be maintained as the
definitions evolve, the process must be automated, without relying on
cut-and-paste or hand-editing of generated \LaTeX{} code.
%
Moreover, it is essential to give the user control of layout.  Here again the
issues of large-scale definitions force our design: in some cases, especially
for small definitions, pretty printing from a prover internal representation can
do a good enough job, but manual formatting choices were necessary to make
(e.g.) our C++11 memory model readable.
%
Accordingly, we preserve all source-file formatting, including line breaks,
indentation, comments, and parentheses, in the generated code.  This lets us
generate corresponding \LaTeX{} code, e.g.~for the previous example:

%Generation of readable output as similar to the original
%input as possible}

%\vspace{-2mm}
\par\noindent{\footnotesize
\lemdef{
\lemkw{let}\  \lemTermVar{consistent\_modification\_order}\  \lemTermVar{actions}\  \lemTermVar{lk}\  \lemTermVar{sb}\  \lemTermVar{sc}\  \lemTermVar{mo}\  \lemTermVar{hb} =\\{}
\ \ (\forall \lemTermVar{a} \mathord{\in}  \lemTermVar{actions}\  \lemTermVar{b} \mathord{\in}  \lemTermVar{actions}.\  ((\lemTermVar{a},\,\lemTermVar{b})\ \in\  \lemTermVar{mo})\  \\{}
\ \ \ \ \lemTermConst{--\mbox{$>$} }\  (\lemTermConst{same\_location}\  \lemTermVar{a}\  \lemTermVar{b}\ \lemwedge\  (\lemTermConst{is\_write}\  \lemTermVar{a}\ \lemwedge\  \lemTermConst{is\_write}\  \lemTermVar{b})))\ \lemwedge\\{}
\ \ (\forall \lemTermVar{l} \mathord{\in}  \lemTermConst{locations\_of}\  \lemTermVar{actions}.\\{}
\ \ \ \ \lemkw{match}\  \lemTermVar{lk}\  \lemTermVar{l} \ \lemkw{with}\  \\{}
\ \ \ \ \ \ \lemTermCtor{Atomic} \rightarrow ( \\{}
\ \ \ \ \ \ \ \ \lemkw{let}\  \lemTermVar{actions\_at\_l} = \\{}
\ \ \ \ \ \ \ \ \ \ \{\lemTermVar{a} | \forall \lemTermVar{a} \mathord{\in}  \lemTermVar{actions} \ |\  \lemTermConst{location\_of}\  \lemTermVar{a}\  \lemTermConst{=}\  \lemTermCtor{Some}\  \lemTermVar{l}\} \ \lemkw{in}\\{}
\ \ \ \ \ \ \ \ \lemkw{let}\  \lemTermVar{writes\_at\_l} = \\{}
\ \ \ \ \ \ \ \ \ \ \{\lemTermVar{a} | \forall \lemTermVar{a} \mathord{\in}  \lemTermVar{actions\_at\_l} \ |\  \lemTermConst{is\_write}\  \lemTermVar{a}\} \ \lemkw{in}\\{}
\ \ \ \ \ \ \ \ \lemTermConst{strict\_total\_order\_over}\  \lemTermVar{writes\_at\_l}\  \\{}
\ \ \ \ \ \ \ \ \ \ (\lemTermConst{restrict\_relation\_set}\  \lemTermVar{mo}\  \lemTermVar{actions\_at\_l})\ \lemwedge\\{}
\ \ \ \ \ \ \ \ \tsholcomm{(* hb is a subset of mo at l *)}\\{}
\ \ \ \ \ \ \ \ (\lemTermConst{restrict\_relation\_set}\  \lemTermVar{hb}\  \lemTermVar{writes\_at\_l}\  \lemTermConst{subset}\  \lemTermVar{mo}\ \lemwedge\\{}
\ \ \ \ \ \ \ \ \tsholcomm{(* SC fences impose mo *)}\\{}
\ \ \ \ \ \ \ \ ((\lemTermConst{restrict\_relation\_set}\\{}
\ \ \ \ \ \ \ \ \ \ (\lemTermConst{compose}\  (\lemTermConst{compose}\  \lemTermVar{sb}\  (\lemTermConst{restrict\_relation\_set}\  \lemTermVar{sc}\  \\{}
\ \ \ \ \ \ \ \ \ \ \ \ \{\lemTermVar{a} | \forall \lemTermVar{a} \mathord{\in}  \lemTermVar{actions} \ |\  \lemTermConst{is\_fence}\  \lemTermVar{a}\}))\  \lemTermVar{sb} )\\{}
\ \ \ \ \ \ \ \ \ \ \lemTermVar{writes\_at\_l})\  \lemTermConst{subset}\  \lemTermVar{mo})))\\{}
\ \ \ \ |\  \_   \rightarrow ( \\{}
\ \ \ \ \ \ \ \ \lemkw{let}\  \lemTermVar{actions\_at\_l} = \\{}
\ \ \ \ \ \ \ \ \ \ \{\lemTermVar{a} | \forall \lemTermVar{a} \mathord{\in}  \lemTermVar{actions} \ |\  \lemTermConst{location\_of}\  \lemTermVar{a}\  \lemTermConst{=}\  \lemTermCtor{Some}\  \lemTermVar{l}\} \ \lemkw{in}\\{}
\ \ \ \ \ \ \ \ \lemTermConst{Set}.\lemTermConst{is\_empty}\  (\lemTermConst{restrict\_relation\_set}\  \lemTermVar{mo}\  \lemTermVar{actions\_at\_l}) ) \\{}
\ \ \ \ \lemkw{end} )
}
}
\noindent It also ensures that the generated prover and OCaml code is
human-readable in its own right.

%\input{Machine-extract.tmp3-tex}
%\dumpallrules

\myparagraph{3. Support for execution}
%a variety of prover and programming language
%targets}
\emph{Exploring} such definitions, and \emph{testing} conformance between
specifications and deployed implementations (and between specifications at
different levels of abstraction), is also a central aspect of our work; both
need some way to make the definitions executable.  In previous work with various
colleagues we have built hand-crafted symbolic evaluators within
HOL4~\cite{BDJRS06,TCP:paper,RNS08,x86popl,TCP:techpaper},
interpreters from code extracted from Coq~\cite{Ctso}, and memory model
exploration tools from code generated from Isabelle/HOL~\cite{C++}.
%
\Lem{} supports several constructs which cannot in general be executed, e.g.,
quantification in propositions and set comprehensions,
but \Lem{} can generate OCaml code where
the range is restricted to a finite set (otherwise OCaml generation
fails).  This has been invaluable for our POWER memory
model exploration tool~\cite{pldi11} and has replaced our use of the Isabelle/HOL code
generator for C++11.

%  prover code extraction
%
%
% %
% The logical design of \Lem{} makes the basic translation to a
% variety of targets straightforward; however, more elaborate support is required
% in the following important cases.
% %
% \begin{itemize}
%
% \item \emph{non-executable definitions\quad}
% %
% \Lem{} supports several constructs for which executable code is not
% immediately available, e.g.,
% quantification in both propositions and set comprehensions.  However, many
% such definitions are not inherently non-executable, and
% \Lem{} can generate OCaml code for
% quantifiers whose range is restricted to a given set; otherwise OCaml generation fails.
% Generally, we find these executable set comprehensions more readable than the
% corresponding iterative OCaml code.
%
% %\item \emph{Partiality.}
% %
% %\Lem{} places no syntactic restrictions on
% %recursive functions definitions (e.g., to primitive recursion),
% %nor does it reject partial pattern matches.
% %Although our intended prover targets are all logics of total functions, they
% %encode partiality differently.  Instead of adopting a particular strategy for the
% %source, we allow any


\myparagraph{4. Quick parsing and type checking with good error messages}
%
This is primarily a matter of careful engineering, using conventional
programming-language techniques.   \Lem{} is a batch-mode tool in the style of
standard compilers, rather than focussed on interactive use, in the typical
proof-assistant style.

%This is a lightweight choice, avoiding the need for an interactive interface,
%and is convenient for definitions split over multiple files, where an update
%may involve coordinated changes to many files.
%
%In contrast to HOL which does lots of model building by proof, and which is
%hobbled by prover representation.
%



%\section{Case study: C++11 Concurrency Model}
\section{C++11 Concurrency in \Lemsect}

We have used \Lem{} to formalise the semantics of the C++11 concurrency
features and its relaxed memory model. As briefly mentioned in
Section~\ref{sub:cppmotivation}, such a formalisation effort has to translate a
specification written in prose English into a concise model written in rigorous
mathematics, which is then used to formally verify specific properties of the
model and the specification---assuming that the model actually models the
specification. Beside pure modelling and verification, previous work has shown
that executing the model is also essential: for understanding the model and
evaluating simple test cases; for checking the model for ``obvious'' errors
before embarking on a hard proof in a theorem prover; and finally for easily
checking the validity of a given execution simply by running the model. 

In case of the C++11 concurrency model, our concrete requirements for a tool
chain are the following:
\begin{itemize}
  \item HOL4 code for proof because of local expertise and to interface with
    previous work;
  \item Ocaml code to simulate the model and to use with the {\sc Cppmem}
    execution checker;
  \item Isabelle/HOL code to interface with the Nitpick counterexample generator;
  \item finally readable \LaTeX{} output for document preparation.
\end{itemize}
\noindent Naturally, adding another tool, such as \Lem{}, to this already long
list is not the only solution to this problem: one can
formalise the model in Isabelle/HOL, for example, and handcraft small,
task-specific tools to produce HOL4 input, to provide additional information to
the Ocaml code generator in Isabelle, to fine-tune the \LaTeX{} output, and so on.

The first iteration of our tool chain, which is used in~\cite{C++}, was created
like this, but resulted in major caveats: eccentricities of each part in the
tool chain forced sub-optimal design choices in the Isabelle definitions, for
example the Isabelle to HOL translator could not handle bounded pairs in
restricted quantifications; and many steps required additional, hand-written
data, such as executable versions of definitions for the Isabelle code generator
or auxiliary data to the HOLDoc type setting tool to produce readable \LaTeX{}.
In aggregate the tool-chain was brittle and unmaintainable; whenever a change to
the model was made, it required pervasive changes to many different files.

%\subsection{Formal semantic definition complements prose specification}
%
%% Our Rube-Goldberg tool-chain
%%
%%    Isabelle Model (Atomic.thy) + codegen axillary file (Code.thy) --{codegen}--> Generated Ocaml (Extracted.ml)
%%       | sed_script
%%    Isabelle ready for Isa2hol
%%       | Isa2hol
%%       `---> Hol model (AtomicScript.sml) + hand written Atomic.imn --{HOLDoc}--> \Latex{} for document production
%%                  | sed_script2
%%                  `---> HOL for proof (AtomicHOLScript.sml)
%
%We needed to construct a tool which would perform the task of checking
%executions for us, but that was not our only requirement of the
%tool-chain we would build. It had to produce:
%
%\begin{itemize}
%\item a HOL4 version to be used as a basis for proof,
%\item readable \LaTeX{} output for document preparation,
%\item Ocaml to be used in an automatic execution checker and
%\item an Isabelle version to test the Nitpick counterexample generator as an execution checker.
%\end{itemize}
%
%In our first iteration of the tool-chain, Isabelle was chosen as the
%formalisation language for its Ocaml output and the potential to use
%the Nitpick solver to act as an in-built execution checker.
%
%\myparagraph{isa2hol generates HOL4 for proofs} We wanted to prove
%that the C++11 concurrency primitives are implementable over a popular
%relaxed memory multiprocessor. We chose the x86 architecture as the
%target because of its popularity and our familiarity with its memory
%model~\cite{tphols09}. Because our x86 memory model, x86-TSO, is
%written in HOL4 and the balance of our expertise is with proof in
%HOL4, we needed a HOL4 version of the C++11 memory model. We created
%\emph{isa2hol}, a lightweight source translator that replaces Isabelle
%syntactic tokens with HOL4 ones. Rather than a general purpose tool,
%isa2hol was specialised to the task, and had many caveats.
%Nonetheless we used the generated HOL4 to prove that for each C++11
%execution, a simple translation of the reads, writes and fences to
%their x86 equivalents provide an execution under the x86-TSO
%memory model that corresponded to a valid execution in the C++11
%model\cite{C++}.
%
%
%\myparagraph{HOLDoc generates \LaTeX{} for documents} As we have
%mentioned, the default typesetting provided by Isabelle is not to our
%tastes, so we decided to use HOLDoc for producing \LaTeX{} from the
%generated HOL4. This required isa2hol to produce a slightly different
%HOL4 version of the model, suppressing parameters to definitions that
%otherwise would have cluttered the output. HOLDoc also required a hand
%written auxiliary file that was specialised to the translated HOL4.
%
%\myparagraph{Isabelle generates Ocaml for {\sc Cppmem}} Isabelle can
%produce Ocaml output, but it required an additional file containing
%executable alternatives of some definitions, workarounds to make types
%in the generated code correct, and commands identifying the
%definitions to pass to the generator. From this we built {\sc Cppmem},
%a tool that exhaustively generates the executions of a given C++11
%program according to the memory model. {\sc Cppmem} is compiled with
%the generated Ocaml. Directly automated translation requires us to
%trust the translator, but is less prone to error than hand translation
%of the model, and changes to the model are automatically propagated to
%the tool. Although {\sc Cppmem} only handles small programs written in
%a restricted subset of the language, it has been extremely valuable
%for testing the model.
%
%\myparagraph{Isabelle uses Nitpick for checking executions} The
%Isabelle model was used with the Nitpick counterexample generator to
%replicate the execution finding functionality of {\sc Cppmem} with
%better performance~\cite{ppdp11}.
%
%Each of the individual parts of the tool-chain had eccentricities that
%forced sub-optimal design choices in the Isabelle definition. For
%example, isa2hol could not translate a bounded pair in restricted
%quantification over a relation, so none were used. In aggregate the
%tool-chain was brittle and unmaintainable; whenever a change to the
%model was made, it required pervasive changes to many different files.
%
%

In the following, we detail key features of \Lem's specification language using
the \Lem{} code of our C++ memory model. We focus on features which are not
necessarily in the familiar intersection of HOL and functional programming, but
features which address our main motivation points and make \Lem{} an adept tool
for formalising large-scale semantics.

\subsection{Backend-specific Definitions}
%
As mentioned before, \Lem's syntax is not restricted to the strict intersection
of all the target languages; such a restriction, like no support for inductive
relations as they are not supported by Ocaml, would make the tool often
practically unusable as a formal specification system.

In our C++ model, the rules of the memory model are stated as predicates on
relations between memory loads and stores, some of which use transitive
closure. Unfortunately, there is no \emph{elegant} transitive closure
definition that lives in the intersection of Ocaml, Isabelle/HOL, and HOL4: the
natural way of defining a transitive closure operator in Ocaml uses recursion.
The recursive definition of a transitive closure, however, cannot be used in
the theorem provers since there is no general termination proof without
additional assumptions.

\Lem{} supports two concepts to solve this problem without any hand-editing of
generated code: backend-specific definitions and substitutions. In the former
case, the user simply parametrises a standard \verb|let| definition or inductive
relation definition with a backend identifier. In the later case, the user
provides a substitution rule that maps a local \Lem{} identifier to a
target-specific identifier in \Lem's target library. We illustrate the use of
both concept using the transitive closure definition of the C++ model. 

In order to provide backend-specific definitions, the type of the identifier has
to be defined first. \Lem{} expects such a type specification when multiple
definitions for a single identifier are made.

\par\noindent{\footnotesize
\begin{verbatim}
val tc : forall 'a. ('a * 'a) set -> ('a * 'a) set
\end{verbatim}
}

\noindent In our C++ model, we choose to use a recursive definition not only for
the OCaml code, but also for generated \LaTeX{} code:

\par\noindent{\footnotesize
\begin{verbatim}
let rec tc r =
  let one_step = { (x,z) | forall ((x,y) IN r) ((y',z) IN r) | y = y' } in
  if one_step subset r then r else
  tc (one_step union r)
\end{verbatim}
}

\noindent Note, that this definition does is neither backend-specific nor a substitution:
\Lem{} supports the use of a \emph{default} definition which is used unless for
there is a backend-specific definition for the currently processed target.

Since HOL4 has a transitive closure definition in its standard library, we
define a substitution to use the library definition in the generated HOL4 code.

\par\noindent{\footnotesize
\begin{verbatim}
sub [hol] tc = tc
\end{verbatim}
}

\noindent It is important to note the semantics of a \verb|sub [target]| statement:
when \Lem{} processes such a statement, it opens its \verb|target| library and
\emph{maps} the right-hand side of the equation into the library scope. In this
case this simply means that \Lem's HOL library has to contain a type definition
for \verb|tc|:\\  
(excerpt from set\_relation.lem, not in the C++ code)

\par\noindent{\footnotesize
\begin{verbatim}
type 'a reln = ('a * 'a) set
val tc : forall 'a. 'a reln -> 'a reln
\end{verbatim}
}

\noindent Using substitution, the generated HOL4 code uses the transitive
closure definition form its own library, which allows the re-use of already
proven properties in the prover code.

Finally, to illustrate backend-specific definitions as well, we state the
transitive closure definition for Isabelle/HOL using inductive relations; we
intensionally do not use an Isabelle/HOL standard library function here.

\par\noindent{\footnotesize
\begin{verbatim}
indreln {isabelle}
  forall r x y. r (x, y) ==> tc' r (x, y) and
  forall r x y. (exist z. tc' r (x,z) && tc' r (z,y)) ==> tc' r (x,y)

let {isabelle} tc r =
 let r' = fun (x,y) -> ((x,y) IN r) in
 { (x,y) | forall ((x,y) IN r) | tc' r' (x,y) }
\end{verbatim}
}

\subsection{Minimal Perturbation in Translation}
%
As previously mentioned, preserving the source layout is one of the main design
goals of \Lem{}: the layout, such as indentation and spacing, is usually
intentional and meant to improve readability.  Therefore, the \Lem{} parser is
able to parse any auxiliary, semantically irrelevant parts of the source code,
such as comments, (redundant) white spaces, and line breaks, and stores them in
the abstract syntax tree with the remaining code.

This ensures that \Lem{} output looks very similar to the original source, no
matter which backend is used. This is a crucial aspect of \Lem{}: only this way,
the generated code is as readable as the source, and one can work in a prover
using the generated code only without needing the \Lem{} source as a reference.
The following, properly indented and commented, excerpt from the C++ \Lem{}
source is used as an example here.\footnote{We use a small font size for these
examples to avoid having to add line breaks.} 

\par\noindent{\scriptsize
\begin{verbatim}
(* CoRR *)
( forall ((x,a) IN rf) ((y,b) IN rf).
  ((a,b) IN hb && same_location a b && is_at_atomic_location lk b) --> 
    ((x = y) || (x,y) IN mo) ) &&
(* CoWR *)    
( forall ((a,b) IN hb) (c IN actions).
  ((c,b) IN rf && is_write a && same_location a b && is_at_atomic_location lk b) -->
    ((c = a) || (a,c) IN mo) ) &&
(* CoRW *)
( forall ((a,b) IN hb) (c IN actions).
  ((c,a) IN rf && is_write b && same_location a b && is_at_atomic_location lk a) --> 
    ((c,b) IN mo) )
\end{verbatim}
}

\noindent The corresponding fragments in the target languages (in order: HOL4,
Isabelle, OCaml, \LaTeX{}) look very similar making the code as legible as the
source. 

\par\noindent{\scriptsize
\begin{verbatim}
(* CoRR *)
( ! ((x,a) :: rf) ((y,b) :: rf).
  ((a,b) IN hb /\ same_location a b /\ is_at_atomic_location lk b) ==> 
    ((x = y) \/ (x,y) IN mo) ) /\ 
(* CoWR *)    
( ! ((a,b) :: hb) (c :: actions).
  ((c,b) IN rf /\ is_write a /\ same_location a b /\ is_at_atomic_location lk b) ==> 
    ((c = a) \/ (a,c) IN mo) ) /\
(* CoRW *)
( ! ((a,b) :: hb) (c :: actions).
  ((c,a) IN rf /\ is_write b /\ same_location a b /\ is_at_atomic_location lk a) ==> 
    ((c,b) IN mo) ))`;
\end{verbatim}
}

\noindent \Lem{} does not generate any x-symbol ASCII strings for
operators in Isabelle, such as \verb|\<and>| for \verb|&|, to preserve
readability of the code in its standard ASCII representation.

\par\noindent{\scriptsize
\begin{verbatim}
(* CoRR *)
(( ALL (x,a) : rf.  ALL (y,b) : rf. 
  ( Set.member (a,b)  hb & same_location a b & is_at_atomic_location lk b) --> 
    ((x = y) | Set.member  (x,y)  mo)) ) & 
(* CoWR *)    
(( ALL (a,b) : hb.  ALL c : actions. 
  ( Set.member (c,b)  rf & is_write a & same_location a b & is_at_atomic_location lk b) --> 
    ((c = a) | Set.member  (a,c)  mo)) )
(* CoRW *)
(( ALL (a,b) : hb.  ALL c : actions. 
  ( Set.member (c,a)  rf & is_write b & same_location a b & is_at_atomic_location lk a) --> 
    ( Set.member (c,b)  mo)) )"
\end{verbatim}
}

\noindent In Ocaml, quantifiers are written without syntactic sugar and the
logically equivalent disjunction is used instead of implication: 

\par\noindent{\scriptsize
\begin{verbatim}
(* CoRR *)
( Pset.for_all (fun (x,a) -> Pset.for_all (fun (y,b) -> (not 
  ( Pset.mem (a,b)  hb && (same_location a b && is_at_atomic_location lk b)) ||  
    ((x = y) || Pset.mem  (x,y)  mo))) rf) rf ) &&
(* CoWR *)    
(( Pset.for_all (fun (a,b) -> Pset.for_all (fun c -> (not 
  ( Pset.mem (c,b)  rf && (is_write a && (same_location a b && is_at_atomic_location lk b))) || 
    ((c = a) || Pset.mem  (a,c)  mo))) actions) hb ) &&
(* CoRW *)
( Pset.for_all (fun (a,b) -> Pset.for_all (fun c -> (not 
  ( Pset.mem (c,a)  rf && (is_write b && (same_location a b && is_at_atomic_location lk a))) ||  
    ( Pset.mem (c,b)  mo))) actions) hb ))
\end{verbatim}
}

\noindent
Also in the \LaTeX{} output, comments, line breaks and indentation match the source:

\par\noindent{\scriptsize
\lemdef{
\lemkw{let}\  \lemTermVar{coherent\_memory\_use}\  \lemTermVar{actions}\  \lemTermVar{lk}\  \lemTermVar{rf}\  \lemTermVar{mo}\  \lemTermVar{hb} =\\{}
\ \ \tsholcomm{(* CoRR *)}\\{}
\ \ ( \forall (\lemTermVar{x},\,\lemTermVar{a}) \mathord{\in}  \lemTermVar{rf}\  (\lemTermVar{y},\,\lemTermVar{b}) \mathord{\in}  \lemTermVar{rf}.\\{}
\ \ \ \ ((\lemTermVar{a},\,\lemTermVar{b})\ \in\  \lemTermVar{hb}\ \lemwedge\  (\lemTermConst{same\_location}\  \lemTermVar{a}\  \lemTermVar{b}\ \lemwedge\  \lemTermConst{is\_at\_atomic\_location}\  \lemTermVar{lk}\  \lemTermVar{b}))\  \lemTermConst{--\mbox{$>$} }\  \\{}
\ \ \ \ \ \ ((\lemTermVar{x}\  \lemTermConst{=}\  \lemTermVar{y})\ \lemvee\  ((\lemTermVar{x},\,\lemTermVar{y})\ \in\  \lemTermVar{mo})) )\ \lemwedge\\{}
\ \ \tsholcomm{(* CoWR *)}    \\{}
\ \ (( \forall (\lemTermVar{a},\,\lemTermVar{b}) \mathord{\in}  \lemTermVar{hb}\  \lemTermVar{c} \mathord{\in}  \lemTermVar{actions}.\\{}
\ \ \ \ ((\lemTermVar{c},\,\lemTermVar{b})\ \in\  \lemTermVar{rf}\ \lemwedge\  (\lemTermConst{is\_write}\  \lemTermVar{a}\ \lemwedge\  (\lemTermConst{same\_location}\  \lemTermVar{a}\  \lemTermVar{b}\ \lemwedge\  \lemTermConst{is\_at\_atomic\_location}\  \lemTermVar{lk}\  \lemTermVar{b})))\  \lemTermConst{--\mbox{$>$} }\\{}
\ \ \ \ \ \ ((\lemTermVar{c}\  \lemTermConst{=}\  \lemTermVar{a})\ \lemvee\  ((\lemTermVar{a},\,\lemTermVar{c})\ \in\  \lemTermVar{mo})) )\ \lemwedge\\{}
\ \ \tsholcomm{(* CoRW *)}\\{}
\ \ ( \forall (\lemTermVar{a},\,\lemTermVar{b}) \mathord{\in}  \lemTermVar{hb}\  \lemTermVar{c} \mathord{\in}  \lemTermVar{actions}.\\{}
\ \ \ \ ((\lemTermVar{c},\,\lemTermVar{a})\ \in\  \lemTermVar{rf}\ \lemwedge\  (\lemTermConst{is\_write}\  \lemTermVar{b}\ \lemwedge\  (\lemTermConst{same\_location}\  \lemTermVar{a}\  \lemTermVar{b}\ \lemwedge\  \lemTermConst{is\_at\_atomic\_location}\  \lemTermVar{lk}\  \lemTermVar{a})))\  \lemTermConst{--\mbox{$>$} }\  \\{}
\ \ \ \ \ \ ((\lemTermVar{c},\,\lemTermVar{b})\ \in\  \lemTermVar{mo}) ))
}
}

\noindent The output is faithful enough to the \Lem{} source to allow proof on
generated sources. Indeed, in work submitted for review \cite{popl12}
we use \Lem{} generated HOL4 as base definitions for proofs.  Our main theorem
shows the redundancy of a particularly complicated rule which
specifies the values of a memory read using existential quantification over a
set of sets of memory writes; omitting this rule results in a significantly
simpler model which is equivalent to the complex one, where equivalence here
refers to both models producing the same set of executions for every C++11
program.


\section{Implementation} \label{sec:impl}

\Lem{} is written in OCaml, and it loosely follows the architecture of a
traditional compiler.  The central data structure is a typed abstract syntax
tree (AST), which \Lem{} builds and processes in 6 steps:

\begin{enumerate}
\item lex and parse the source files into untyped ASTs;
\item type check the ASTs, and convert them into typed ASTs;
\item transform the typed ASTs with target-specific macros;
\item rename variables and top level definitions, as required by the target;
\item add extra parentheses and remove infix operators, as required by the target;
\item print the resulting AST in the target's syntax.
\end{enumerate}

\myparagraph{Lexing and parsing}
\Lem{}'s input parser is built with \texttt{ocamllex} and \texttt{ocamlyacc},
which are OCaml's implementations of the Lex and Yacc lexer and parser
generators.  \Lem{}'s lexer and parser depart from standard practice by
preserving all spaces, line breaks and comments of the input, so that they can
be included in the output.  The lexer tracks these characters and returns them
with their following token.  The parser then incorporates them into the untyped
AST.  For example, when processing the following definition, the comment
%
\verb|(* Define x *)|
%
and a line break are returned with the \texttt{let} token.  Furthermore, a
single space is returned with each of \texttt{x} and \texttt{=}, and a line
break and two spaces are returned with the \texttt{1} token.

{
\par\noindent{\footnotesize
\begin{verbatim}
(* Define x *)
let x =
  1
\end{verbatim}
}}

\noindent The OCaml type declarations for the untyped AST are automatically extracted
from the formal definition of \Lem{}'s syntax by Ott, to help us keep the
implementation and formal specification in agreement.  We also test \Lem{}'s
handling of spaces by checking that parsed (and type checked) input can be
converted to character-by-character identical output.

\myparagraph{Type checking}
\Lem{}'s type checker uses a standard approach based on constraint
unification~\cite[Section 3]{typeconstraints}, where a constraint either
requires two types to be equal, or requires a type to be an instance of a type
class.  No local constraints are used, and generalization only occurs for
top-level definitions (e.g., nested \texttt{let}s are not polymorphic), so a
single global store of type class instantiation information is sufficient.
Constraints are required to have unique solutions, so that no type can be a
member of a particular class in multiple ways.  Instantiations can give rise to
implicational constraints, e.g., type \texttt{'a list} is a member of the
\texttt{Eq} class whenever its parameter \texttt{'a} is.

The implementation generally follows the formal type system (which is specified
in Ott); however, it must also disambiguate some nodes of the untyped AST.  The
`\texttt{.}' character is used both for projection from modules and records,
and so type information must be used to determine which is which.  This differs
from OCaml where a variable referring to a record must start with a lower case
letter, and a module name must start with an upper case one, allowing for a
purely syntactic disambiguation.  In \Lem{}, variables can start with either
upper or lower case letters.

If the input is well typed, the type checker produces a AST that maintains all
of the lexical information on whitespace and comments, and also annotates every
sub-expression with its type.

\myparagraph{Transformation}
The various output targets do not support all of the features of \Lem{}.
Thus, \Lem{} must remove any unsupported expression or definition forms and
replace them with equivalent supported ones.  It does this for each output
target separately.  For example, OCaml does not support set or list
comprehensions, and so those are translated into explicit iterations using fold
operators (as long as the comprehension variables are given explicit ranges; the
translation must fail otherwise).  In HOL4, set comprehensions are supported,
but list comprehensions are not, so only the latter is translated.

The transformation system is structured as a LISP-style macro expander.  That
is, it makes a top-down pass over each expression and at each subexpression it
checks if there are any macros that apply.  If there are, it applies the first
to get a new subexpression, and then repeats the check and apply until there are
none.  It then continues the traversal using the newly generated subexpression.
This design allows each transformation to be written separately (and so easily
used in different combinations for different back ends), and without repeatedly
writing AST traversal code.  Because it operates on the typed AST, the result of
each macro is required to have the same type as its input.

\paragraph{Dictionary passing} Neither HOL4 nor OCaml support type classes, and
so \Lem{} must be able to remove them.  It uses a simple dictionary passing
translation~\cite{ocaml-typeclass} where each function with a constrained type
parameter gets an extra ``dictionary'' argument that contains implementations of
the class' functions (e.g., the \texttt{=} function of the \texttt{Eq} class)
for the parameterised type.  Because constructor classes (e.g., monads) are not
supported, the dictionary itself is typeable in \Lem{}'s type system.  In
cases where a class member function is called on a concrete type, no dictionary
is needed.  For example, in the function \texttt{let f x = (x = 2)}, the
implementation of \texttt{=} for numbers can be referred to directly without
giving \texttt{f} an extra argument.

\paragraph{Substitution} The standard libraries of the various targets have
differing data representations and interfaces, but to avoid the proliferation of
back-end specific specifications, \Lem{} only supports a single interface
for each desired feature (e.g., finite maps, or bit vectors).  \Lem{}'s
standard library specifies how to translate from its own representation to each
back end using substitution functions that are inlined during the transformation
process.  For example, the following excerpt from the list library specifies
that each occurrence of \texttt{List.length} translates to \texttt{LENGTH} in
HOL4, and to \texttt{List.length} in Isabelle/HOL and OCaml.

{
\par\noindent{\footnotesize
\begin{verbatim}
val length : forall 'a. 'a list -> num
sub [hol] length = LENGTH
sub [ocaml] length = List.length
sub [isabelle] length = List.length
\end{verbatim}
}}
\noindent
Each of these substitutions is type checked using \Lem{} types for the various
target system libraries to ensure that the substitution generates type-correct
code.  This improves on Ott's similar \emph{hom} functionality, which performs
target specific substitutions, but does no type checking, possibly leading to
output code with type errors whose source was not apparent in output since it
arose from a bad substitution.


\myparagraph{Renaming}
Although \Lem{}, OCaml, HOL4, and Isabelle/HOL have similar scoping rules,
and are all based on $\lambda$-calculus, they have significant differences in
how names are treated.  Furthermore the macro transformation system is not
hygienic or referentially transparent~\cite{ref-trans-macro,hygienic-macro},
possibly leading to inadvertent name capture or collision.  A target-specific
renaming pass ensures that each variable will refer to the correct local
variable or top-level definition when when the target system processes the
generated output file.

For example, top-level names become special in HOL4 and Isabelle/HOL and cannot
be used as parameters in function definitions.  Thus, the following definition
is invalid:

{
\par\noindent{\footnotesize
\begin{verbatim}
let x = 1
let add1 x = x + 1
\end{verbatim}}
}
\noindent
and must be renamed:

{
\par\noindent{\footnotesize
\begin{verbatim}
let x = 1
let add1 x0 = x0 + 1
\end{verbatim}}}

\noindent Similar trouble can occur for HOL4 if we use \texttt{LENGTH} as a parameter
name.

{
\par\noindent{\footnotesize
\begin{verbatim}
let eq_length l LENGTH = (List.length l = LENGTH)
\end{verbatim}}}

\noindent
This must be renamed so that the \texttt{LENGTH} function that comes from the
library substitution of \texttt{LENGTH} (see above) is not captured by the local
variable.

{
\par\noindent{\footnotesize
\begin{verbatim}
let eq_length l LENGTH0 = (LENGTH l = LENGTH0)
\end{verbatim}}
}
\noindent Lastly, HOL4 and Isabelle/HOL do not support nested modules, and so they must be
flattened while avoiding the creation of conflicting names.

\myparagraph{Parenthesis insertion}
Just as the target systems have subtly different scoping rules, they also have
different parsing rules.  \Lem{} contains a model of the precedences that
each target system assigns to its infix operators, and it adds extra parentheses
wherever the output would not be parsed correctly.  It avoids adding unnecessary
parentheses, so that the output will resemble the input as much as possible.
This step happens after the transformation step so that the macros need not
avoid introducing infix operations.

\myparagraph{Printing}
All of the complex processing is performed on typed abstract syntax trees in the
above stages, keeping the printing step as simple as possible, and somewhat
uniform across the various target.  Instead of using a pretty printing algorithm
for layout, \Lem{} uses the spacing from the input.  Furthermore, a model
of the lexical structure of the target is used to insert extra spaces wherever
required.  This frees the macro transformations from having to worry about
whether to insert spacing or not.


\section{Future work}

We are actively developing \Lem{}: our immediate goal is to polish the existing
backends (and finish the in-progress Coq backend).  Subsequently, we plan to add
more backends, e.g., for Haskell or ACL2, and to re-engineer Ott to use \Lem{}
as a translation target, as mentioned above. This way, Off's current ad-hoc code
generator for Isabelle/HOL, Coq, and HOL4 can be replaced.
%
In order to increase the \Lem{} code base and to use existing work more
efficiently, we are also planing on looking at translators to import existing
code to \Lem{}: a HOL4-to-\Lem{} translation---allowing us to automatically
port, for example, Fox's detailed ARM instruction semantics~\cite{anthonyARM} to
other provers---as well as Coq-to-\Lem{} and Isabelle/HOL-to-\Lem{} translations
which will need expertise in the front-end implementations of those systems.
%
\Lem{} does not currently support OCaml generation for inductively defined
relations (although one can sometimes use the Isabelle backend and then apply
its code generation mechanism).  Ultimately, we would like to directly generate
OCaml that searches for derivations; this will be particularly useful in
conjunction with Ott, for running test and example programs directly on an
operational semantics.

Although \Lem{} is primarily a design and engineering project, it would benefit
from a rigorous understanding of exactly how the semantics of the source and
target logics relate to each other, for the fragments we consider.  In
particular, when multiple provers are used to verify properties of a
\Lem{}-specified system, we would like a semantic justification that the
resulting definitions have the same meaning, and that a lemma verified in one
prover can be used in another.
%
There have been several projects that port low-level proofs between provers (a
very different problem to the readable-source-file porting that we consider
here); while this approach yields the right guarantees, we expect it would be
very challenging because the various backends can transform the same definition
differently (e.g., keeping type classes for Isabelle, but not for HOL4).

%Mention proof porting and lemma statements.



%\vspace*{-3mm}


\bibliographystyle{splncs03}
%\bibliographystyle{alpha}
{
\bibliography{vstte12}
}
\end{document}


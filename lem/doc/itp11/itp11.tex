%!TEX TS-program = personallatex

\documentclass[a4paper]{llncs}

\usepackage[T1]{fontenc}
\usepackage{ae}
\usepackage{hyperref}
\usepackage{lem}
\usepackage{url}

\newcommand{\myparagraph}[1]{\vspace{0.5\baselineskip}\par\noindent{\normalsize\bfseries{#1}}\quad}

\makeatletter
\renewcommand{\section}{\vspace*{-2mm}\@startsection{section}{1}{\z@}%
                       {-18\p@ \@plus -4\p@ \@minus -4\p@}%
                       {4\p@ \@plus 2\p@ \@minus 2\p@}%
                       {\normalfont\large\bfseries\boldmath
                        \rightskip=\z@ \@plus 8em\pretolerance=10000 }}
\makeatother

%\pagestyle{plain}

\newcommand{\Sevcik}{\v{S}ev\v{c}\'{\i}k}

%\newcommand{\toolname}{Ichor}
%\newcommand{\toolname}{LM}
\newcommand{\toolname}{\textsc{Lem}}
%\newcommand{\toolname}{LEM}
\newcommand{\toolnamesect}{Lem}
\newcommand{\toolurl}{\url{www.cl.cam.ac.uk/~so294/lem/}}


\author{Scott Owens\inst{1} \and Peter B\"ohm\inst{1} \and Francesco Zappa Nardelli\inst{2} \and Peter Sewell\inst{1}}
\title{Lem: A Lightweight Tool for Heavyweight Semantics}
\institute{University of Cambridge \and INRIA\\ \url{www.cl.cam.ac.uk/users/so294/lem}\vspace*{-3mm}}


\begin{document}
\sloppy
\maketitle
\vspace*{-3mm}
\begin{abstract}
Many ITP developments exist in the context of a single prover, and are
dominated by proof effort.  In contrast, when applying rigorous semantic
techniques to realistic computer systems, engineering the
definitions becomes a major activity in its own right.  Proof is then
only one task among many: testing, simulation, communication, community
review, etc.  Moreover, the effort invested in establishing such
definitions should be re-usable and, where possible, irrespective of the
local proof-assistant culture.  For example, in recent work on processor
and programming language concurrency (x86, Power, ARM, C++0x,
CompCertTSO), we have used Coq, HOL4, Isabelle/HOL, and Ott---often
using multiple provers simultaneously, to exploit existing definitions or
local expertise.   

In this paper we describe \toolname{}, a prototype system specifically designed to
support pragmatic engineering of such definitions.  It has a carefully
designed source language, of a familiar higher-order logic with datatype
definitions, inductively defined relations, and so on.  This is
typechecked and translated to a variety
of programming languages and proof assistants, preserving the
original source structure (layout,  comments, etc.) so that the result
is readable and usable.  We have already found this invaluable in our
work on Power, ARM and C++0x concurrency.
\end{abstract}

\vspace*{-7mm}

\section{Motivation}
Mechanised proof assistants %and \emph{interactive theorem
%provers} 
such as ACL2~\cite{ACL2}, Coq~\cite{Coq}, HOL4~\cite{HOL}, HOL
Light~\cite{HOLlight}, Isabelle/HOL~\cite{Isabelle}, PVS~\cite{PVS}, and
Twelf~\cite{Twelf} are becoming important tools for Computer Science.
In many applications of these tools, the majority of effort is devoted
to proof, and that is rightly a main focus of their developers.  This
focus leads each of these systems to have its own logic, various mechanisms for
making mathematical definitions, and extensive support for machine-checked interactive and/or automated
reasoning.

In some applications, however, the \emph{definitions} themselves, of
types, functions, and relations, are a
major focus of the work.  This is often the case when modelling key
computational infrastructure: network protocols, programming languages,
multiprocessors, and so on.  For example, we have worked on
TCP~\cite{TCP:paper,RNS08}, Optical Networking~\cite{BDJRS06},
%TCP:techpaper,
Java Module Systems~\cite{ljam}, the semantics of an OCaml
fragment~\cite{ocamllightesop}, concurrency for C and
C++~\cite{C++,ppdp11,Ctso}, and the semantics of x86, POWER and ARM
multiprocessors~\cite{pldi11,x86popl}; and there are numerous
examples by other groups (far too many to cite here).  In each of
these cases, considerable effort was required to establish the
definitions of syntax and semantics, including analysis of informal
specifications, empirical testing, and proof of metatheory.  
%
These definitions can be large: for example, our TCP specification is
around 10\,000 non-comment lines of HOL4.  At this scale, the activity
of working with the definitions becomes more like developing software than
defining small calculi: one has to refactor, test, coordinate between
multiple people, and so on, and all of this should, as far as
possible, be complete before one embarks on any proof.  


  Moreover, in such work a
proof assistant is just one piece of a complex project, involving
production typesetting, testing infrastructure, code generation, and
tools for embedding source-language terms into the prover.  Sometimes
there is no proof activity, but great benefits arise simply from working
in typechecked and typeset mathematics; sometimes there is mechanised
symbolic evaluation or code generation for testing and prototyping;
sometimes there is hand proof or a mixture of hand and mechanised proof;
and sometimes there is the classic full mechanised proof supported by
provers.

Ideally, the results of such work should be made widely available in a
\emph{re-usable} form, so that other groups can build on them and so
that the field can eventually converge on standard models for the
relatively stable aspects of the computational environment in which we
work.   
%
%
Unfortunately, at present such re-use is highly restricted for two
reasons.
%\begin{itemize}
%\item
Firstly, the field is partitioned into schools around each prover: the
difficulty in becoming fluent in their use means that very few people
can use more than one tool effectively.  Indeed, even within some of our
own projects we have had to use several provers due to differing local
expertise.  This variation makes it hard to compare the results of even
carefully specified benchmarks, such as the POPLmark
challenge.%~\cite{poplmark}.

%\item
Secondly, the differences between the provers mean that it is a major
and error-prone task to port a development---or even just its
definitions---from one system to another.  In some cases this is for
fundamental reasons:   definitions which make essential use
of the dependent types of Coq may be hard or impossible to practically
port to HOL4.  However, many of the examples cited above are 
logically undemanding: they have no need for dependent types, the
differences between classical and constructive reasoning are not
particularly relevant, and there is often little or no object-language
variable binding (of course this does not apply for formalisation of
rich type theories).   They do make heavy use of basic discrete mathematics and
``programming language'' features: sets and set comprehensions;
first-order logic; and inductive types and records with functions and
relations over them.  Thus, the challenge is one of robustly
translating between the concrete syntax and definition styles of the
different proof assistants. 
%\end{itemize}

\section{Portable definitions with \toolnamesect{}}

%
%In previous work, we developed the Ott tool for expressing programming
%language definitions~\cite{ott}, that takes definitions of language
%syntax and semantics, expressed in a readable domain-specific
%metalanguage, and compiles them to Coq, HOL4, Isabelle/HOL, and LaTeX;
%it was used for the Java Modules and OCaml semantics work cited above.
%This does provide reusability and prover-independence, but is not
%easily applicable outside the specific domain of programming language
%definitions. 

We have designed a language, \emph{\toolname{}}, for writing,
managing, and publishing large scale semantic definitions, 
for use as an intermediate language when generating definitions
from domain-specific tools, and for use as an
intermediate language for porting definitions between existing provers.
%
  Our
implementation can currently typecheck \toolname{} sources, and
%, for a
%significant portion of the language, 
generate HOL4, Isabelle/HOL, OCaml, and \LaTeX{} (the latter drawing
on Wansborough's HOLDoc tool design).
Development of a Coq backend is in progress.  We are already
using \toolname{} in our research: we developed a semantics for
multiprocessor concurrency on the POWER architecture~\cite{pldi11} in \toolname{}, and our semantics
for C++0x~\cite{C++,ppdp11} concurrency has been ported from Isabelle/HOL to \toolname{}.

Semantically, we have designed \toolname{} to be 
roughly the intersection of
common functional programming languages and higher-order logics,
%(especially HOL4, Isabelle/HOL, and Coq)
as we regard this as a sweet spot: expressive enough for the
applications we mention above, yet familiar and relatively easy to translate 
into the various provers; there is intentionally no logical novelty here. 
%
\toolname{} 
%is similar to HOL and Isabelle/HOL: it 
has a simple
type theory %~\cite{church40}
with primitive support for recursive and
higher-order functions, inductive relations, n-ary tuples, algebraic
datatypes, record types, type inference, and top-level polymorphism.  
It also includes a type class mechanism
broadly similar to Isabelle's and Haskell's %~\cite{haskell}
(without
constructor classes).  It differs from the internal logics of HOL4 or
Isabelle/HOL principally in having type, function and relation
definitions as part of the language rather than encoded into it:
the \toolname{} type system is formally defined (using Ott~\cite{ott}) in terms of
the user-level syntax. 


The novelty is rather in the detailed design and implementation,
%The novelty is rather in the details of design and implementation,
to ensure the following four important pragmatic properties. 
%Roughly, the first three are achieved through careful
%design of the source language, and the remaining four also require good
%engineering of the implementation.  
We can achieve all of these
goals more easily than one could in context of a  prover implementation because we are not constrained to
use an intermediate representation suitable for the implementation of a
proof kernel (e.g., explicitly typed lambda terms), and because we
are building a lightweight stand-alone tool, without a large legacy codebase.



\myparagraph{1. Readability of source files}
\toolname{} syntactically resembles OCaml and F\texttt{\#}, %~\cite{fsharp},
giving us a popular and readable syntax.
It includes nested modules (but not functors), 
recursive type and function definitions, record types, type abbreviations, and
pattern matching.
  It has additional
syntax for quantifiers, including restricted quantifiers ($\forall x \in S.\ P x$),
set comprehension, and inductive relations.
For example, here is an extract from our POWER model:

\vspace*{-2.2mm}
\par\noindent{\footnotesize
\begin{verbatim}
let write_reaching_coherence_point_action m s w = 
  let writes_past_coherence_point' =
    s.writes_past_coherence_point union {w} in
  (* make write before other writes to this address not past coherence *)
  let coherence' = s.coherence union
      { (w,wother) | forall (wother IN (writes_not_past_coherence s)) 
      | (not (wother = w)) && (wother.w_addr = w.w_addr)}  in
  <| s with coherence = coherence';
            writes_past_coherence_point = writes_past_coherence_point' |> 
let sem_of_instruction i ist =
  match i with
  | Padd  set rD rA rB  -> op3regs   Add set rD rA rB ist
  | Psub  set rD rA rB  -> op3regs   Sub set rD rB rA ist (* swap args *)
  end    
\end{verbatim}

}

\vspace*{-2.2mm}

% 
% 
% type action = 
%   | Read_reg of reg * value
%   | Write_reg of reg * value
% 
% let op3regs op set rD rA rB ist = 
%   let (vA,ist') = fresh_var Unit ist in
%   let (vB,ist'') = fresh_var Unit ist' in
%   let (v,ist''') = fresh_var Unit ist'' in
%   match set with
%   | DontSetCR0 ->
%       (<|empty_sem with
%        remaining = [Read_reg rA vA ;
%                     Read_reg rB vB ;
%                     Binop  v op vA vB ;
%                     Write_reg rD v ]|>,
%         ist''')
%         
% \end{verbatim}
% }
% {\small
% \begin{verbatim}
% let visible_sequence_of_side_effects_tail actions mo hb vsse_head b =
%     { c | forall (c IN actions) |
%       (vsse_head,c) IN mo && not ((b,c) IN hb) &&
%       ( forall (a IN actions).
%           (vsse_head,a) IN mo && (a,c) IN mo ===> not ((b,a) IN hb) ) }
% \end{verbatim}
%
%
%
%(e.g., Microsoft's F\texttt{\#}~\cite{fsharp} uses OCaml's syntax).

\noindent We do not always follow OCaml: for example, \toolname{} 
uses curried data constructors
instead of tupled ones, and it uses \verb+<|+ and \verb+|>+ for records, 
saving \verb|{| and \verb|}| for set comprehensions.
%
Type classes provide principled support for overloading.
% S: Actually, we don't yet support Unicode.
%, and we support Unicode.

\toolname{} does not at present include support for arbitrary
user-defined syntax, as provided by Ott~\cite{ott} and (to a greater or lesser
extent) by several proof assistants.  
%
\toolname{} and Ott have complementary strengths:  Ott is particularly
useful for defining semantics as inductively defined relations over a
rich user syntax, but has limited support for logic, sets, and
function definitions, whereas \toolname{} is the converse.  
We envisage refactoring the Ott implementation, which currently
generates
Coq, HOL, and Isabelle/HOL code separately, to instead generate
\toolname{} code and leave the prover-specific output to the \toolname{}
tool.  In the longer term, a metalanguage that combines both is highly desirable.





\myparagraph{2. Taking the source text seriously}
\emph{Explaining} the definitions is a key aspect of the kind of work we mention above.
%
We need to produce production-quality typesetting, of
the complete definitions in logical order and of various excerpts, in
papers, longer documents, and presentations.  As all these have to be
maintained as the definitions evolve, the process must be automated,
without relying on cut-and-paste or hand-editing of generated LaTeX
code.
%
Moreover, it is essential to give the user control of layout.
Here again the issues of large-scale definitions force our design: in
some cases, especially for small definitions, pretty printing from a
prover internal representation can do a good enough job, but manual formatting choices were necessary to make
(e.g.) our C++0x memory model readable. 
%
Accordingly, we preserve all
source-file formatting, including line breaks, indentation, comments, and parentheses,
in the generated code.  This lets us generate corresponding LaTeX code, e.g.~for the previous example:

%Generation of readable output as similar to the original
%input as possible}

\vspace{-2mm}
\par\noindent{\footnotesize

\lemdef{
\lemkw{let}\  \lemTermVar{write\_reaching\_coherence\_point\_action}\  \lemTermVar{m}\  \lemTermVar{s}\  \lemTermVar{w} = \\{}
\ \ \lemkw{let}\  \lemTermVar{writes\_past\_coherence\_point}' =\\{}
\ \ \ \ \lemTermVar{s}.\lemTermField{writes\_past\_coherence\_point}\ \cup\  \{\lemTermVar{w}\} \ \lemkw{in}\\{}
\ \ \tsholcomm{(* make write before other writes to this address not past coherence *)}\\{}
\ \ \lemkw{let}\  \lemTermVar{coherence}' = \lemTermVar{s}.\lemTermField{coherence}\ \cup\\{}
\ \ \ \ \ \ \{ (\lemTermVar{w},\,\lemTermVar{wother}) | \forall \lemTermVar{wother} \mathord{\in}  (\lemTermConst{writes\_not\_past\_coherence}\  \lemTermVar{s}) \\{}
\ \ \ \ \ \ |\  (\lemnot\  (\lemTermVar{wother}\  \lemTermConst{=}\  \lemTermVar{w}))\ \lemwedge\  (\lemTermVar{wother}.\lemTermField{w\_addr}\  \lemTermConst{=}\  \lemTermVar{w}.\lemTermField{w\_addr})\}  \ \lemkw{in}\\{}
\ \ \Mlrec \lemTermVar{s} \ \Mmagicwith\  \lemTermField{coherence} = \lemTermVar{coherence}';\\{}
\ \ \ \ \ \ \ \ \ \ \ \ \lemTermField{writes\_past\_coherence\_point} = \lemTermVar{writes\_past\_coherence\_point}' \Mmagicrrec
}

\lemdef{
\lemkw{let}\  \lemTermVar{sem\_of\_instruction}\  \lemTermVar{i}\  \lemTermVar{ist} =\\{}
\ \ \lemkw{match}\  \lemTermVar{i} \ \lemkw{with}\\{}
\ \ |\  \lemTermCtor{Padd}\   \lemTermVar{set}\  \lemTermVar{rD}\  \lemTermVar{rA}\  \lemTermVar{rB}  \rightarrow \lemTermConst{op3regs}\     \lemTermCtor{Add}\  \lemTermVar{set}\  \lemTermVar{rD}\  \lemTermVar{rA}\  \lemTermVar{rB}\  \lemTermVar{ist}\\{}
\ \ |\  \lemTermCtor{Psub}\   \lemTermVar{set}\  \lemTermVar{rD}\  \lemTermVar{rA}\  \lemTermVar{rB}  \rightarrow \lemTermConst{op3regs}\     \lemTermCtor{Sub}\  \lemTermVar{set}\  \lemTermVar{rD}\  \lemTermVar{rB}\  \lemTermVar{rA}\  \lemTermVar{ist}\ \tsholcomm{(* swap args *)}\\{}
\ \ \lemkw{end}
}

}

\vspace{-2mm}
\noindent
It also ensures that the generated prover and OCaml code is human-readable in its own right.







%\input{Machine-extract.tmp3-tex}
%\dumpallrules

\myparagraph{3. Support for execution}
%a variety of prover and programming language
%targets}
\emph{Exploring} such definitions, and \emph{testing} conformance
between specifications and deployed implementations (and between
specifications at different levels of abstraction), is also a central
aspect of our work; both need some way to make the definitions
executable.
 In previous work with various colleagues 
we have built
hand-crafted symbolic evaluators within
HOL4~\cite{BDJRS06,TCP:paper,RNS08,x86popl},
%TCP:techpaper,
interpreters from code extracted from Coq~\cite{Ctso},
and memory model exploration tools from code generated from
Isabelle/HOL~\cite{C++}.
%
\toolname{} supports several constructs which cannot in general be executed, e.g., 
quantification in propositions and set comprehensions,
but \toolname{} can generate OCaml code where
the range is restricted to a finite set (otherwise OCaml generation
fails).  This has been invaluable for our POWER memory
model exploration tool~\cite{pldi11}.


%  prover code extraction 
% 
% 
% %
% The logical design of \toolname{} makes the basic translation to a
% variety of targets straightforward; however, more elaborate support is required
% in the following important cases. 
% %
% \begin{itemize}
% 
% \item \emph{non-executable definitions\quad}
% %
% \toolname{} supports several constructs for which executable code is not
% immediately available, e.g., 
% quantification in both propositions and set comprehensions.  However, many
% such definitions are not inherently non-executable, and
% \toolname{} can generate OCaml code for 
% quantifiers whose range is restricted to a given set; otherwise OCaml generation fails.  
% Generally, we find these executable set comprehensions more readable than the 
% corresponding iterative OCaml code.
% 
% %\item \emph{Partiality.}
% %
% %\toolname{} places no syntactic restrictions on 
% %recursive functions definitions (e.g., to primitive recursion),
% %nor does it reject partial pattern matches.
% %Although our intended prover targets are all logics of total functions, they
% %encode partiality differently.  Instead of adopting a particular strategy for the
% %source, we allow any





\myparagraph{4. Quick parsing and type checking with good error
messages}
%
This is primarily a matter of careful engineering, using conventional
programming-language techniques.   \toolname{} is a
batch-mode tool in the style of standard compilers, rather than focussed
on interactive use, in the typical proof-assistant style.  

%This is a lightweight choice, avoiding 
%the need for an interactive interface, and is convenient for definitions split over multiple files, where an update may involve coordinated changes to many files. 

%
%
%In contrast to HOL which does lots of model building by proof, and which is hobbled by prover representation.
%




\section{Implementation}
%
Our \toolname{} implementation is written in OCaml, using Ott to specify
the concrete syntax, and it loosely follows the architecture of a
traditional compiler.  The central data structure is a typed
abstract syntax tree (AST), and processing follows 4 phases:
%
%\begin{enumerate}
%\item 
(1) source files are lexed and parsed into untyped ASTs;
(2) the untyped ASTs are type checked and converted into typed ASTs;
(3) typed-AST-to-typed-AST transformations remove language features
that are not present in the target (e.g., the removal of type classes by
introducing dictionary passing for OCaml and HOL4); and 
(4) the transformed, typed AST is printed in the target language syntax.
%
%\end{enumerate}
%
We try to make the printing step as simple as possible, and
uniform across the various back-ends, by handling all of the complexities
of translation in (3).  The untyped and typed ASTs
contain all of the whitespace (both indentation and line breaks)
and comments of the original source file; the step (4) printer uses these
instead of a pretty printing algorithm for layout.

%Comment on how even LCF-style provers use unverified parsers and pretty printers.
%
%
%
The logical design of \toolname{} makes the basic translation to a
variety of targets straightforward.
%
% however, more elaborate support is required
%in the following important cases. 
%
The standard libraries of our various targets have differing data representations
and interfaces.  For each desired feature (e.g., finite maps, or bit vectors), 
we design an interface for \toolname{}, and specify how that interface is 
to be translated for each target.
This is similar to Ott's \emph{hom} functionality; however, 
here we typecheck the translation specifications to ensure that
the generated code is well-formed.


%\end{itemize}


\section{Future work}

We are actively developing \toolname{}: our immediate goal is to
finish and polish the existing backends (including the in-progress Coq backend).
%Subsequently, we plan to add more backends, e.g., for
%Haskell, and 
%plan to re-engineer Ott to use  \toolname{} as
%a translation target, as mentioned above. 
%including a.  
%We would like to re-engineer Ott's backend via \toolname{},
%replacing Ott's current ad-hoc code generator for Isabelle/HOL, Coq and HOL4.
Also of
interest is a HOL4-to-\toolname{} translation (allowing us to
automatically port, for example, Fox's detailed ARM instruction semantics~\cite{anthonyARM}
to other provers) and we 
would like Coq-to-\toolname{} and Isabelle/HOL-to-\toolname{}
translations,
which will need expertise in the front-end implementations of those
systems. 
%
\toolname{} does not currently support OCaml generation for
inductively defined relations (although one can sometimes use the Isabelle backend and then
apply its code generation mechanism).  Ultimately, we would like to directly generate
OCaml that searches for derivations; this will be particularly useful in conjunction with Ott, for
running test and example programs directly on an operational semantics.

Although \toolname{} is primarily a design and engineering project, it
would benefit from a rigorous understanding of exactly how the semantics
of the source and target logics relate to each other, for the
fragments we consider.  In particular, when
multiple provers are used to verify properties of a \toolname{}-specified system,
we would like a semantic justification that the resulting definitions have the
same meaning, and that a lemma verified in one prover can be used in another.
%
There have been several projects that port low-level proofs between
provers (a very different problem to the readable-source-file porting
that we consider here); while
this approach yields the right guarantees, we expect it would be
very challenging because the various backends can transform the same definition
differently (e.g., keeping type classes for Isabelle, but not for HOL4).

%Mention proof porting and lemma statements.



\vspace*{-3mm}


\bibliographystyle{splncs03}
%\bibliographystyle{alpha}
{
\bibliography{itp11}
}
\end{document}

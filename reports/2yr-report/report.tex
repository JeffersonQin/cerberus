%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[11pt,a4paper, twocolumn]{article}

\usepackage[british]{babel}
\bibliographystyle{alpha}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

% \usepackage{xltxtra}
%   \setmainfont[Mapping=tex-text, Ligatures={Common}, Numbers={OldStyle}]{Adobe Garamond Pro}
%   \setmonofont[Scale=0.72, Ligatures=NoCommon]{Monaco}
%   \setsansfont[Scale=0.9, Ligatures=NoCommon]{Gill Sans}
%   \defaultfontfeatures{Scale=MatchLowercase}

% \usepackage{unicode-math}
%   \setmathfont{STIXGeneral}

\usepackage[colorlinks, citecolor=citeBlue]{hyperref}
\usepackage{url}

\addtolength{\voffset}{-2cm}
\addtolength{\hoffset}{-1.5cm}
\addtolength{\textwidth}{3cm}
\addtolength{\textheight}{4cm}

\usepackage{color}
\definecolor{citeBlue}{rgb}{0,0,1}
% WIP colors
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{red}{rgb}{1,0,0}
\definecolor{blue}{rgb}{0,0,0.5}


\newcommand{\ail}{\textsf{Ail}}
\newcommand{\core}{\textsf{Core}}
\newcommand{\theC}{\textsf{C}}
\newcommand{\Csem}{\textsf{Csem}}

\newcommand{\IBMPOWER}{IBM POWER}
\newcommand{\POWER}{POWER}
\newcommand{\Coq}{Coq}
\newcommand{\Lem}{Lem}
\newcommand{\OCaml}{OCaml}
\newcommand{\HOL}{HOL4}
\newcommand{\Isabelle}{Isabelle}
\newcommand{\LLVM}{LLVM}
\newcommand{\GCC}{GCC}
\newcommand{\CCured}{CCured}

% \newcommand{\IBMPOWER}{\textsf{IBM POWER}}
% \newcommand{\POWER}{\textsf{POWER}}
% \newcommand{\Coq}{\textsf{Coq}}
% \newcommand{\Lem}{\textsf{Lem}}
% \newcommand{\OCaml}{\textsf{OCaml}}
% \newcommand{\HOL}{\textsf{HOL4}}
% \newcommand{\Isabelle}{\textsf{Isabelle}}
% \newcommand{\LLVM}{\textsf{LLVM}}
% \newcommand{\GCC}{\textsf{GCC}}
% \newcommand{\CCured}{\textsf{CCured}}


\newcommand{\TODO}{\textsc{ToDo}}


\title{Second Year Report \& Dissertation Schedule}
\author{Kayvan Memarian}
\date{August 2013}


\begin{document}

\maketitle

\section*{Summary}

The project laid out in my first year report was the development of an
executable formal semantics for the C programming language. While
there exists past work on this topic, and several formalizations of C
have been produced, the present work focuses on two aspects which have
not really been dealt with in the past. First we consider that a
formal model of the C language should not make any assumption
regarding implementation defined choices (aspects of the language that
the C standard leaves up to the implementor to define). Second, we
would like a semantics capable of dealing with realistic low level
programs, like pre-existing fragments of operating systems.

To answer to this problem, I have opted for a {\it elaborative}
approach, where the semantics of C is given through a translation
function to a simpler and more principled language (called Core)
designed specifically for this purpose, and for which a direct
semantics is given.

Most of my work during my second year has been focused on fixing the
design for the Core language, adding successively new features as they
where needed to allow the writing of an accurate elaboration function
from C. Briefly these additions include: proper procedures (as opposed
to the original pure functions); a labeling and jump statement; a
return statement; a shifting operator (to allow the modification of
memory location values); more complicated values to deal with
struct/union and function pointers, reserved symbolic names for
implementation-defined constants.


\section*{Research activity}

\subsection*{Implementing Core and adding control operators}

During the first month following the submission of my first year
report, I focused on producing an executable specification of Core's
dynamics in the \texttt{Lem} specification language. This was
integrated with Csem, a tool started by Justus Matthiesen that I took
over during my first year.

The tool can be thought of as a C compiler and evaluator: it takes as
input a preprocessed C program; translates it into a intermediate
language very close to the actual C; performs C type-checking and type
annotate the program; elaborates it into a Core program; and finally
the Core implementation evaluates the generated Core program. The tool
can also run in an exhaustive search mode, finding all the possible
ways of executing the source program. Soon there will also be a way of
extracting, from execution traces, pre-executions that can then be fed
to Mark Batty's concurrency memory model.

My original design of Core did not provide special constructs for the
elaboration of C's control features like loops, switches, gotos and in
particular the \texttt{break} and \texttt{continue} statements. The
original plan was to model these features purely through the use of
recursive Core functions. As a result I first implemented Core's
dynamics following a traditional small-step style. While that approach
kept the dynamics of Core simple (there was no need for continuations)
and was nicely compositional, it had the undesirable side-effect of
having the elaboration function from C to Core introduce a significant
amount of code duplication and also lead to a loss of the original
program structure through the heavy use of auxiliary functions
(e.g. for each \texttt{return} statement in a C function, one would
need to create a separate Core function containing the elaboration of
the prefix of that C function up to that particular
\texttt{return}). As an alternative I added to Core a pair of
constructs similar to labeled and goto statements. However, since
memory objects, and in particular their lifetime, are made precise in
the syntax of Core programs, the labelling and goto constructs need an
involved semantics including the creation and termination of
objects. For example, in C one can have the following situation:

\begin{verbatim}
int *p;
{
  int x;
  p = &x;
  goto label;
}

...

label:
  S;
\end{verbatim}

\noindent Here, when the execution enters the block (the region
surrounded by braces), a new memory object is created, corresponding
to \texttt{x}, and the following assignment makes the global pointer
\texttt{p} point to that memory object. Then the \texttt{goto}
statement jumps out of the block, at the same time ending the lifetime
of that local memory object. At this point, the C standard says that
the value of the pointer become {\it indeterminate}. This is precisely
the sort of thing we aim to accurately model with our semantics. To do
so, we therefore need to somehow propagate through the C to Core
elaboration some information regarding block-scope C variables. This
cannot be done directly as Core does not provide memory references
(which is C's ``variables'' really are); instead we have symbolic
names, some of them corresponding to the memory locations of objects
(e.g. in the elaboration of the beginning of the lifetime of a C
object, a specific Core memory allocating action is performed and its
value is bound to a fresh symbolic name). As a result, Core's labeling
and goto statements are annotated with the relevant symbolic names in
a way allowing us to simulate lambda lifting while at the same time
preserving the original program structure.

A side effect of this new approach to modelling C's control operator
is a loss of some compositionally at the level of C functions, since
the elaboration from C to Core now needs to pre-calculate the
continuations associated to labels (this is made unavoidable by
forward jumps) and the relation between block scoped object and Core
symbolic names.

Another related construct added to Core is one analogous to C's
\texttt{return}. The addition of these three constructs radically
changed the shape of Core's dynamics which is now implemented as a
small-step evaluator working on stack of continuations.


\subsection*{Indeterminately sequenced expressions}

In the first year report, I mentioned C function calls and the fact
that regarding their ordering the C standard says that they are ``{\it
  indeterminately-sequenced} with actions for which other operators do
not already enforce any ordering''. This combines with the semantics
of the postfix increment and decrement operators, whose two actions
(load and incrementing/decrementing store) are defined to be atomic
from the point of view of an indeterminately-sequence function
call. To model this, Core has the special ordering operator ($\rhd$),
for the elaboration of increments/decrements, and the unary operator
($\{ \cdot \}$), marking indeterminately-sequenced expression.

When we want to actually run a Core expression containing some
indeterminate sequencing, we need to enumerate all possible induced
sequencing graphs. For example, in \texttt{x++~+~\{ E \}} there are
two possible sequencing graphs: one where the expression E occurs
before the increment, and one where it occurs after.

One way to solve this problem would be to use a graph semantics of
Core expressions (actions are nodes; the sequencing operator adds an
edge from the nodes of its first operand to the nodes of its second
operand; the unsequencing operator adds no edge; \dots); isolate
indeterminately sequenced actions and find all the ways new edges can
be added to the graph to make everything strictly ordered. The problem
here is again the lack of compositionality. Instead, I wrote a
compositional Core to Core rewriting system which takes as input a
Core expression that may make use of the ($\rhd$) and ($\{ \cdot \}$)
operators and produce a set of Core expressions without these (but
possibly adding the use of a non-deterministic choice operator). This
transformation fits in the pipeline of Csem and comes after a simple
type-checker for Core but before the evaluator.


\subsection*{Tool engineering}

I have also spent a fair amount of my time doing general software
engineering on Csem. We used to make use of our own C parser (other
works tend to make use of the CIL framework, but we preferred not to
rely on it as its parser does not follow clearly what the standard
describes) but we did not support struct/union, typedef, compound
expressions and the multitude of ways to write constants. Since making
a C complete parser is notoriously difficult, we opted to adopt a
trustworthy parser produced in the context of the Compcert project.

I have also worked on adding to Csem the infrastructure that will
allow us to use it as a tool for detecting whether programs depend on
implementation-defined behaviours, by adding a Core standard library
and the mechanism to import implementation-defined choices definition
files.


\subsection*{Memory layout model}

From the point of view of our semantic model of C, the dynamics of
Core described earlier allow us to specify what memory actions occurs
during the evaluation of C expressions and statements, and what
ordering constraints must be respected. But this leaves out the
details of where and how the memory objects are stored, accessed and
modified. This aspect is dealt with by a memory layout model. This can
be made into a component of our tool Csem that remains independent of
the other components (like the C type-checker, or the dynamics of C
implemented by the elaboration function and Core's implementation).

The development of such a memory model for Csem has been another focus
of my work. The main challenge comes from the fact that the C standard
tries to describe a model which on one side, guarantees some high
level properties (somehow matching what is usually assumed on works
doing proof on C like programs); while on the other side C has
\texttt{unsigned char} pointers which basically allow programs to
manipulate the memory at the level of bits. The bit-level aspect of
the memory model is however further complicated by the fact almost all
details are left implementation defined. A distinction is made between
values and bit representation, the latter being possibly non-unique
nor fixed during an execution. Finally there seems to be no consensus
regarding what the C memory model is among standard writers, compiler
implementors and C programmers. This was in particular made apparent
to us, after we created a quiz about the memory model and add it
filled by various C peoples (including C standard authors, C
implementors, expert and non-expert C programmers). This quiz was
made of 38 questions, each presenting a short C program and asking the
participant to first give the behaviour they expected the program to
have, then to say what they believed the standard said about it.

As result I have been working on a flexible model aiming at matching
the different views of the memory model via a combination of
switches. The approach we follow tries have a representation of the
memory as abstract as possible (in particular because of our objective
of making the model independent of implementation defined choices)
with fallback to concrete evaluations when really required.

At the moment, the memory model only exists as a mix of prose and
non-operational Lem code. The current version of Csem relies on a
naive memory model (map from memory objects to values) which has
sufficient to evaluate ordering related part of the semantics and
simple expressions. In the short term, I am planning to polish and
translate to operational Lem code our real memory layout to be able to
connect it to the rest of Csem.


\subsection*{}

In the short term, I plan polish the Csem tool. In particular a number
of holes remain in the elaboration function, and add the memory model
as just described. Once this is done, I will be able to do large scale
testing of the semantics (so far I have only relied on small hand
written programs). This will probably make use of Regehr's Csmith
tool.

Towards the goal of producing from Csem a tool for the detection of
program dependency on implementations, I plan to modify the Core
evaluator by adding a symbolic evaluator. This will allow us to
continue program execution until implementation-defined constant are
really needed.

Another short term project is to connect Csem to an operational
version of Mark Batty's concurrency memory model that has be developed
last year by Kyndylan Nienhuis.


\subsection*{Thesis layout}

Here follows a preliminary plan for my thesis structure:
\begin{enumerate} \itemsep0em 
\item An introductory chapter on the origins of the C programming
  language; detailing how it is defined through the ISO standard and
  the problems arising from this (in particular the notion of
  undefined behaviours and implementation-defined choices). Review of
  previous work on formal semantics for C.

\item A presentation of our elaborative approach for building a formal
  semantics for C. This chapter would details the separation of tasks
  among the different intermediate languages I use (statics and
  type-checking, elaboration of the dynamics, and actual
  execution). Details would in particular be given about the structure
  of the elaboration function from C to Core.

\item A chapter dedicated to presenting the syntax and semantics of
  the Core intermediate language, by going through its constructs and
  motivating them by relating them to the C constructs requiring them
  in their elaboration.

\item A short chapter would present the structure of the Csem
  development and discuss the software engineering aspects.

\item A chapter would be dedicated to the C memory model. It would
  start by presenting the notions defined in the C standard. Then a
  discussion about the disconnection between the standard and actual
  practise would follow. Finally our formalisation would be presented
  and motivated.

\item A chapter would be dedicated to the tool I plan to develop
  from Csem for the detection of how given C programs depends on
  implementation choices.

\item A chapter would present how one can use the elaboration function
  and Core's semantics to reason about C to C program
  transformations. The content of this chapter and the previous one is
  research that remains to be done.
\end{enumerate}

\end{document}
